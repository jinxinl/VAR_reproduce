






=======================================================   RESTART [03-24 08:55:48]   =======================================================
[03-24 08:55:48] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 08:55:48] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 08:55:48] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True







=======================================================   RESTART [03-24 08:55:48]   =======================================================







=======================================================   RESTART [03-24 08:55:48]   =======================================================







=======================================================   RESTART [03-24 08:55:48]   =======================================================







=======================================================   RESTART [03-24 08:55:48]   =======================================================







=======================================================   RESTART [03-24 08:55:48]   =======================================================







=======================================================   RESTART [03-24 08:55:48]   =======================================================







=======================================================   RESTART [03-24 09:05:09]   =======================================================







=======================================================   RESTART [03-24 09:05:09]   =======================================================







=======================================================   RESTART [03-24 09:05:09]   =======================================================







=======================================================   RESTART [03-24 09:05:09]   =======================================================







=======================================================   RESTART [03-24 09:05:09]   =======================================================







=======================================================   RESTART [03-24 09:05:09]   =======================================================







=======================================================   RESTART [03-24 09:05:09]   =======================================================







=======================================================   RESTART [03-24 09:05:09]   =======================================================
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:05:09] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True







=======================================================   RESTART [03-24 09:14:55]   =======================================================
[03-24 09:14:55] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:14:55] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:14:55] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:14:55] (/root/VAR/train.py      , line  36)=> global bs=768, local bs=768
[03-24 09:14:55] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 0.00030000000000000003
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 768
  batch_size          : 768
  glb_batch_size      : 768
  ac                  : 1
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=768 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b768ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 09:14:55] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 09:14:58] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 09:14:58] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 09:14:58] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 09:14:58] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 09:14:58] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 09:14:58] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7ff24a43f310>
[03-24 09:14:58] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 09:14:58] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 09:14:58] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 09:14:58] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 09:14:58] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 09:14:58] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7ff24a43f310>
[03-24 09:14:58] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 09:14:58] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 09:14:58] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 09:14:58] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 09:14:58] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=768, lbs=768, iters_train=1669, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 09:14:58] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 09:14:59] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422







=======================================================   RESTART [03-24 09:18:37]   =======================================================
[03-24 09:18:37] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:18:37] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:18:37] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:18:37] (/root/VAR/train.py      , line  36)=> global bs=768, local bs=768
[03-24 09:18:37] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 0.00030000000000000003
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 768
  batch_size          : 768
  glb_batch_size      : 768
  ac                  : 1
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=768 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b768ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 09:18:37] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 09:18:40] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 09:18:40] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 09:18:40] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 09:18:40] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 09:18:40] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 09:18:40] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7fd1ae6c6310>
[03-24 09:18:40] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 09:18:40] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 09:18:40] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 09:18:40] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 09:18:40] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 09:18:40] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7fd1ae6c6310>
[03-24 09:18:40] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 09:18:40] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 09:18:40] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 09:18:40] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 09:18:40] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=768, lbs=768, iters_train=1669, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 09:18:41] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 09:18:41] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422







=======================================================   RESTART [03-24 09:30:12]   =======================================================
[03-24 09:30:12] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:30:12] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:30:12] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:30:12] (/root/VAR/train.py      , line  36)=> global bs=768, local bs=768
[03-24 09:30:12] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 0.00030000000000000003
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 768
  batch_size          : 768
  glb_batch_size      : 768
  ac                  : 1
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=768 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b768ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 09:30:12] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 09:30:15] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 09:30:15] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 09:30:15] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 09:30:15] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 09:30:15] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 09:30:15] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f11a53b8310>
[03-24 09:30:15] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 09:30:15] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 09:30:15] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 09:30:15] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 09:30:15] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 09:30:15] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f11a53b8310>
[03-24 09:30:15] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 09:30:15] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 09:30:15] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 09:30:15] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 09:30:15] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=768, lbs=768, iters_train=1669, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 09:30:16] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 09:30:16] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 09:30:47] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 09:30:47] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 09:30:47] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 09:30:47] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 09:30:47] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 09:30:47] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 09:30:47] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 0.00030000000000000003, 'weight_decay': 0}








=======================================================   RESTART [03-24 09:39:52]   =======================================================
[03-24 09:39:52] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:39:52] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:39:52] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:39:52] (/root/VAR/train.py      , line  36)=> global bs=768, local bs=384
[03-24 09:39:52] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 0.00030000000000000003
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 768
  batch_size          : 384
  glb_batch_size      : 768
  ac                  : 1
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=768 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b768ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 09:39:52] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 09:39:55] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 09:39:55] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 09:39:55] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 09:39:55] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 09:39:55] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 09:39:55] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7fd705679280>
[03-24 09:39:55] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 09:39:55] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 09:39:55] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 09:39:55] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 09:39:55] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 09:39:55] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7fd705679280>
[03-24 09:39:55] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 09:39:55] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 09:39:55] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 09:39:55] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 09:39:55] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=768, lbs=384, iters_train=1669, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 09:39:56] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 09:39:56] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 09:39:57] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 09:39:57] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 09:39:57] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 09:39:57] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 09:39:57] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 09:39:57] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 09:39:57] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 0.00030000000000000003, 'weight_decay': 0}








=======================================================   RESTART [03-24 09:51:12]   =======================================================
[03-24 09:51:12] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 09:51:12] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 09:51:12] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 09:51:12] (/root/VAR/train.py      , line  36)=> global bs=96, local bs=48
[03-24 09:51:12] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 7.500000000000001e-05
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 96
  batch_size          : 48
  glb_batch_size      : 96
  ac                  : 2
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=192 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1 --ac=2
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b96ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 09:51:12] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 09:51:15] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 09:51:15] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 09:51:15] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 09:51:15] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 09:51:15] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 09:51:15] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f7a27871280>
[03-24 09:51:15] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 09:51:15] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 09:51:15] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 09:51:15] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 09:51:15] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 09:51:15] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f7a27871280>
[03-24 09:51:15] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 09:51:15] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 09:51:15] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 09:51:15] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 09:51:15] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=96, lbs=48, iters_train=13346, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 09:51:15] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 09:51:15] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 09:51:16] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 09:51:16] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 09:51:16] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 09:51:16] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 09:51:16] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 09:51:16] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 09:51:16] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 7.500000000000001e-05, 'weight_decay': 0}








=======================================================   RESTART [03-24 10:09:22]   =======================================================
[03-24 10:09:22] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 10:09:22] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 10:09:22] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 10:09:22] (/root/VAR/train.py      , line  36)=> global bs=96, local bs=48
[03-24 10:09:22] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 7.500000000000001e-05
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 96
  batch_size          : 48
  glb_batch_size      : 96
  ac                  : 2
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=192 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1 --ac=2
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b96ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 10:09:22] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 10:09:25] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 10:09:25] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 10:09:25] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:09:25] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 10:09:25] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:09:25] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f2d59eb5280>
[03-24 10:09:25] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:09:25] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 10:09:25] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:09:25] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 10:09:25] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:09:25] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f2d59eb5280>
[03-24 10:09:25] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:09:25] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 10:09:25] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 10:09:25] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 10:09:25] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=96, lbs=48, iters_train=13346, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 10:09:25] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 10:09:26] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 10:09:27] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 10:09:27] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 10:09:27] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 10:09:27] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 10:09:27] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 10:09:27] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 10:09:27] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 7.500000000000001e-05, 'weight_decay': 0}

[03-24 10:09:31] (/root/VAR/trainer.py    , line 122)=> loss: 8.317854881286621







=======================================================   RESTART [03-24 10:13:00]   =======================================================
[03-24 10:13:00] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 10:13:00] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 10:13:00] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 10:13:00] (/root/VAR/train.py      , line  36)=> global bs=96, local bs=48
[03-24 10:13:00] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 7.500000000000001e-05
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 96
  batch_size          : 48
  glb_batch_size      : 96
  ac                  : 2
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=192 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1 --ac=2
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b96ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 10:13:00] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 10:13:03] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 10:13:03] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 10:13:03] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:13:03] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 10:13:03] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:13:03] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7fe1f54cf280>
[03-24 10:13:03] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:13:03] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 10:13:03] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:13:03] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 10:13:03] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:13:03] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7fe1f54cf280>
[03-24 10:13:03] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:13:03] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 10:13:03] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 10:13:03] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 10:13:03] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=96, lbs=48, iters_train=13346, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 10:13:03] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 10:13:04] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 10:13:04] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 10:13:04] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 10:13:04] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 10:13:04] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 10:13:04] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 10:13:04] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 10:13:04] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 7.500000000000001e-05, 'weight_decay': 0}

[03-24 10:13:09] (/root/VAR/trainer.py    , line 122)=> loss: 8.317852020263672
[03-24 10:13:09] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>







=======================================================   RESTART [03-24 10:15:18]   =======================================================







=======================================================   RESTART [03-24 10:15:18]   =======================================================







=======================================================   RESTART [03-24 10:15:18]   =======================================================







=======================================================   RESTART [03-24 10:15:18]   =======================================================
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 10:15:18] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True







=======================================================   RESTART [03-24 10:20:41]   =======================================================
[03-24 10:20:41] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 10:20:41] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 10:20:41] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 10:20:41] (/root/VAR/train.py      , line  36)=> global bs=96, local bs=48
[03-24 10:20:41] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 7.500000000000001e-05
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 96
  batch_size          : 48
  glb_batch_size      : 96
  ac                  : 2
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=192 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1 --ac=2
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b96ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 10:20:41] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 10:20:44] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 10:20:44] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 10:20:44] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:20:44] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 10:20:44] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:20:44] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7fa3b73bc1f0>
[03-24 10:20:44] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:20:44] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 10:20:44] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:20:44] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 10:20:44] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:20:44] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7fa3b73bc1f0>
[03-24 10:20:44] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:20:44] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 10:20:44] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 10:20:44] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 10:20:44] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=96, lbs=48, iters_train=13346, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 10:20:44] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 10:20:45] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 10:20:46] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 10:20:46] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 10:20:46] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 10:20:46] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 10:20:46] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 10:20:46] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 10:20:46] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 7.500000000000001e-05, 'weight_decay': 0}

[03-24 10:20:46] (/root/VAR/train.py      , line 297)=> *********g_it: 0
[03-24 10:20:46] (/root/VAR/train.py      , line 298)=> *********ep: 0
[03-24 10:20:46] (/root/VAR/train.py      , line 299)=> *********iters_train: 13346
[03-24 10:20:46] (/root/VAR/train.py      , line 300)=> *********it: 0
[03-24 10:20:46] (/root/VAR/train.py      , line 303)=> *********stepping: False
[03-24 10:20:50] (/root/VAR/trainer.py    , line 122)=> loss: 8.317892074584961
[03-24 10:20:50] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>







=======================================================   RESTART [03-24 10:43:43]   =======================================================
[03-24 10:43:43] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 10:43:43] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 10:43:43] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 10:43:43] (/root/VAR/train.py      , line  36)=> global bs=96, local bs=48
[03-24 10:43:43] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 7.500000000000001e-05
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 96
  batch_size          : 48
  glb_batch_size      : 96
  ac                  : 2
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=192 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1 --ac=2
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b96ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 10:43:43] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 10:43:46] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 10:43:46] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 10:43:46] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:43:46] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 10:43:46] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:43:46] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f3b03440310>
[03-24 10:43:46] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:43:46] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 10:43:46] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:43:46] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 10:43:46] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:43:46] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f3b03440310>
[03-24 10:43:46] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:43:46] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 10:43:46] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 10:43:46] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 10:43:46] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=96, lbs=48, iters_train=13346, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 10:43:46] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 10:43:47] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 10:43:48] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 10:43:48] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 10:43:48] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 10:43:48] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 10:43:48] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 10:43:48] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 10:43:48] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 7.500000000000001e-05, 'weight_decay': 0}

[03-24 10:43:52] (/root/VAR/trainer.py    , line 122)=> loss: 8.317773818969727
[03-24 10:43:52] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:43:53] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [    0/13346]  eta: 18:53:15  tlr: 3.8e-07  tnm: 0.00  Lm: 8.318 (8.318)  Lt: 8.318 (8.318)  Accm: 0.03 (0.03)  Acct: 0.02 (0.02)  time: 5.0948  data: 0.5136
[03-24 10:43:54] (/root/VAR/trainer.py    , line 122)=> loss: 8.317731857299805
[03-24 10:43:54] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:43:57] (/root/VAR/trainer.py    , line 122)=> loss: 8.317956924438477
[03-24 10:43:57] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:43:58] (/root/VAR/trainer.py    , line 122)=> loss: 8.317802429199219
[03-24 10:43:58] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:00] (/root/VAR/trainer.py    , line 122)=> loss: 8.31778335571289
[03-24 10:44:00] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:02] (/root/VAR/trainer.py    , line 122)=> loss: 8.317873001098633
[03-24 10:44:02] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:04] (/root/VAR/trainer.py    , line 122)=> loss: 8.317728042602539
[03-24 10:44:04] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:05] (/root/VAR/trainer.py    , line 122)=> loss: 8.317947387695312
[03-24 10:44:05] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:07] (/root/VAR/trainer.py    , line 122)=> loss: 8.317634582519531
[03-24 10:44:07] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:09] (/root/VAR/trainer.py    , line 122)=> loss: 8.317719459533691
[03-24 10:44:09] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:11] (/root/VAR/trainer.py    , line 122)=> loss: 8.317705154418945
[03-24 10:44:11] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:12] (/root/VAR/trainer.py    , line 122)=> loss: 8.317646026611328
[03-24 10:44:12] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:14] (/root/VAR/trainer.py    , line 122)=> loss: 8.317672729492188
[03-24 10:44:14] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:16] (/root/VAR/trainer.py    , line 122)=> loss: 8.317806243896484
[03-24 10:44:16] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:18] (/root/VAR/trainer.py    , line 122)=> loss: 8.317811965942383
[03-24 10:44:18] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:19] (/root/VAR/trainer.py    , line 122)=> loss: 8.317811965942383
[03-24 10:44:19] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:21] (/root/VAR/trainer.py    , line 122)=> loss: 8.31775188446045
[03-24 10:44:21] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:23] (/root/VAR/trainer.py    , line 122)=> loss: 8.317705154418945
[03-24 10:44:23] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:24] (/root/VAR/trainer.py    , line 122)=> loss: 8.317740440368652
[03-24 10:44:24] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:26] (/root/VAR/trainer.py    , line 122)=> loss: 8.317642211914062
[03-24 10:44:26] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:28] (/root/VAR/trainer.py    , line 122)=> loss: 8.317760467529297
[03-24 10:44:28] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:30] (/root/VAR/trainer.py    , line 122)=> loss: 8.317671775817871
[03-24 10:44:30] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:31] (/root/VAR/trainer.py    , line 122)=> loss: 8.317588806152344
[03-24 10:44:31] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:33] (/root/VAR/trainer.py    , line 122)=> loss: 8.317611694335938
[03-24 10:44:33] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:35] (/root/VAR/trainer.py    , line 122)=> loss: 8.317535400390625
[03-24 10:44:35] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:37] (/root/VAR/trainer.py    , line 122)=> loss: 8.317646026611328
[03-24 10:44:37] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:39] (/root/VAR/trainer.py    , line 122)=> loss: 8.317564010620117
[03-24 10:44:39] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:40] (/root/VAR/trainer.py    , line 122)=> loss: 8.317495346069336
[03-24 10:44:40] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:42] (/root/VAR/trainer.py    , line 122)=> loss: 8.31764030456543
[03-24 10:44:42] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:44] (/root/VAR/trainer.py    , line 122)=> loss: 8.317462921142578
[03-24 10:44:44] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:46] (/root/VAR/trainer.py    , line 122)=> loss: 8.317526817321777
[03-24 10:44:46] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:47] (/root/VAR/trainer.py    , line 122)=> loss: 8.317498207092285
[03-24 10:44:47] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:49] (/root/VAR/trainer.py    , line 122)=> loss: 8.317598342895508
[03-24 10:44:49] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:51] (/root/VAR/trainer.py    , line 122)=> loss: 8.317487716674805
[03-24 10:44:51] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:53] (/root/VAR/trainer.py    , line 122)=> loss: 8.317541122436523
[03-24 10:44:53] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:54] (/root/VAR/trainer.py    , line 122)=> loss: 8.317472457885742
[03-24 10:44:54] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:56] (/root/VAR/trainer.py    , line 122)=> loss: 8.317489624023438
[03-24 10:44:56] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:44:58] (/root/VAR/trainer.py    , line 122)=> loss: 8.317469596862793
[03-24 10:44:58] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:00] (/root/VAR/trainer.py    , line 122)=> loss: 8.31745719909668
[03-24 10:45:00] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:01] (/root/VAR/trainer.py    , line 122)=> loss: 8.317502975463867
[03-24 10:45:01] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:03] (/root/VAR/trainer.py    , line 122)=> loss: 8.317513465881348
[03-24 10:45:03] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:05] (/root/VAR/trainer.py    , line 122)=> loss: 8.317438125610352
[03-24 10:45:05] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:07] (/root/VAR/trainer.py    , line 122)=> loss: 8.317439079284668
[03-24 10:45:07] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:08] (/root/VAR/trainer.py    , line 122)=> loss: 8.317390441894531
[03-24 10:45:08] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:10] (/root/VAR/trainer.py    , line 122)=> loss: 8.317465782165527
[03-24 10:45:10] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:12] (/root/VAR/trainer.py    , line 122)=> loss: 8.31744384765625
[03-24 10:45:12] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:14] (/root/VAR/trainer.py    , line 122)=> loss: 8.317367553710938
[03-24 10:45:14] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:15] (/root/VAR/trainer.py    , line 122)=> loss: 8.317389488220215
[03-24 10:45:15] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:17] (/root/VAR/trainer.py    , line 122)=> loss: 8.317266464233398
[03-24 10:45:17] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:19] (/root/VAR/trainer.py    , line 122)=> loss: 8.317435264587402
[03-24 10:45:19] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:21] (/root/VAR/trainer.py    , line 122)=> loss: 8.317241668701172
[03-24 10:45:21] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:22] (/root/VAR/trainer.py    , line 122)=> loss: 8.317243576049805
[03-24 10:45:22] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:24] (/root/VAR/trainer.py    , line 122)=> loss: 8.317339897155762
[03-24 10:45:24] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:26] (/root/VAR/trainer.py    , line 122)=> loss: 8.317249298095703
[03-24 10:45:26] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>
[03-24 10:45:28] (/root/VAR/trainer.py    , line 122)=> loss: 8.317386627197266
[03-24 10:45:28] (/root/VAR/trainer.py    , line 123)=> <class 'torch.Tensor'>







=======================================================   RESTART [03-24 10:45:48]   =======================================================
[03-24 10:45:48] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 10:45:48] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 10:45:48] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 10:45:48] (/root/VAR/train.py      , line  36)=> global bs=96, local bs=48
[03-24 10:45:48] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 7.500000000000001e-05
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 96
  batch_size          : 48
  glb_batch_size      : 96
  ac                  : 2
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=192 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1 --ac=2
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b96ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 10:45:48] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 10:45:51] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 10:45:51] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 10:45:51] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:45:51] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 10:45:51] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:45:51] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f4d8aa54310>
[03-24 10:45:51] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:45:51] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 10:45:51] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:45:51] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 10:45:51] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:45:51] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f4d8aa54310>
[03-24 10:45:51] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:45:51] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 10:45:51] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 10:45:51] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 10:45:51] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=96, lbs=48, iters_train=13346, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 10:45:51] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 10:45:52] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 10:45:53] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 10:45:53] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 10:45:53] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 10:45:53] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 10:45:53] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 10:45:53] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 10:45:53] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 7.500000000000001e-05, 'weight_decay': 0}

[03-24 10:45:58] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [    0/13346]  eta: 19:18:14  tlr: 3.8e-07  tnm: 0.00  Lm: 8.318 (8.318)  Lt: 8.318 (8.318)  Accm: 0.01 (0.01)  Acct: 0.02 (0.02)  time: 5.2071  data: 0.5728







=======================================================   RESTART [03-24 10:59:10]   =======================================================
[03-24 10:59:10] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 10:59:10] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 10:59:10] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 10:59:10] (/root/VAR/train.py      , line  36)=> global bs=96, local bs=48
[03-24 10:59:10] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 7.500000000000001e-05
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 96
  batch_size          : 48
  glb_batch_size      : 96
  ac                  : 2
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=192 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1 --ac=2
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b96ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 10:59:10] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 10:59:13] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 10:59:13] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 10:59:13] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:59:13] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 10:59:13] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:59:13] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f511be42310>
[03-24 10:59:13] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:59:13] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 10:59:13] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 10:59:13] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 10:59:13] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 10:59:13] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f511be42310>
[03-24 10:59:13] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 10:59:13] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 10:59:13] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 10:59:13] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 10:59:13] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=96, lbs=48, iters_train=13346, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 10:59:14] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 10:59:14] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 10:59:15] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 10:59:15] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 10:59:15] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 10:59:15] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 10:59:15] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 10:59:15] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 10:59:15] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 7.500000000000001e-05, 'weight_decay': 0}

[03-24 10:59:19] (/root/VAR/trainer.py    , line 122)=> loss: 8.317925453186035
[03-24 10:59:20] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [    0/13346]  eta: 19:55:21  tlr: 3.8e-07  tnm: 0.00  Lm: 8.318 (8.318)  Lt: 8.318 (8.318)  Accm: 0.02 (0.02)  Acct: 0.02 (0.02)  time: 5.3740  data: 0.4996







=======================================================   RESTART [03-24 11:04:50]   =======================================================
[03-24 11:04:50] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 11:04:50] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 11:04:50] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 11:04:50] (/root/VAR/train.py      , line  36)=> global bs=96, local bs=48
[03-24 11:04:50] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 7.500000000000001e-05
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 96
  batch_size          : 48
  glb_batch_size      : 96
  ac                  : 2
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=192 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1 --ac=2
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b96ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 11:04:50] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 11:04:53] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 11:04:53] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 11:04:53] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 11:04:53] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 11:04:53] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 11:04:53] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f2f1aeb3310>
[03-24 11:04:53] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 11:04:53] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 11:04:53] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 11:04:53] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 11:04:53] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 11:04:53] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f2f1aeb3310>
[03-24 11:04:53] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 11:04:53] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 11:04:53] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 11:04:53] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 11:04:53] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=96, lbs=48, iters_train=13346, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 11:04:54] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 11:04:54] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 11:04:55] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 11:04:55] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 11:04:55] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 11:04:55] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 11:04:55] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 11:04:55] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 11:04:55] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 7.500000000000001e-05, 'weight_decay': 0}

[03-24 11:04:59] (/root/VAR/trainer.py    , line 122)=> iter: 0	loss: 8.317858695983887
[03-24 11:05:00] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [    0/13346]  eta: 19:10:55  tlr: 3.8e-07  tnm: 0.00  Lm: 8.318 (8.318)  Lt: 8.318 (8.318)  Accm: 0.02 (0.02)  Acct: 0.01 (0.01)  time: 5.1743  data: 0.6086
[03-24 11:05:02] (/root/VAR/trainer.py    , line 122)=> iter: 1	loss: 8.317825317382812
[03-24 11:05:04] (/root/VAR/trainer.py    , line 122)=> iter: 2	loss: 8.317770004272461
[03-24 11:05:06] (/root/VAR/trainer.py    , line 122)=> iter: 3	loss: 8.317702293395996
[03-24 11:05:07] (/root/VAR/trainer.py    , line 122)=> iter: 4	loss: 8.317739486694336
[03-24 11:05:09] (/root/VAR/trainer.py    , line 122)=> iter: 5	loss: 8.317787170410156
[03-24 11:05:11] (/root/VAR/trainer.py    , line 122)=> iter: 6	loss: 8.317657470703125
[03-24 11:05:13] (/root/VAR/trainer.py    , line 122)=> iter: 7	loss: 8.317758560180664
[03-24 11:05:15] (/root/VAR/trainer.py    , line 122)=> iter: 8	loss: 8.317739486694336
[03-24 11:05:16] (/root/VAR/trainer.py    , line 122)=> iter: 9	loss: 8.317720413208008
[03-24 11:05:18] (/root/VAR/trainer.py    , line 122)=> iter: 10	loss: 8.317764282226562
[03-24 11:05:20] (/root/VAR/trainer.py    , line 122)=> iter: 11	loss: 8.317679405212402
[03-24 11:05:22] (/root/VAR/trainer.py    , line 122)=> iter: 12	loss: 8.317621231079102
[03-24 11:05:23] (/root/VAR/trainer.py    , line 122)=> iter: 13	loss: 8.317691802978516
[03-24 11:05:25] (/root/VAR/trainer.py    , line 122)=> iter: 14	loss: 8.317691802978516
[03-24 11:05:26] (/root/VAR/trainer.py    , line 122)=> iter: 15	loss: 8.317734718322754







=======================================================   RESTART [03-24 11:06:05]   =======================================================
[03-24 11:06:05] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 11:06:05] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 11:06:05] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 11:06:05] (/root/VAR/train.py      , line  36)=> global bs=96, local bs=48
[03-24 11:06:05] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 7.500000000000001e-05
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 96
  batch_size          : 48
  glb_batch_size      : 96
  ac                  : 2
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=192 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1 --ac=2
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b96ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 11:06:05] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 11:06:08] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 11:06:08] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 11:06:08] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 11:06:08] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 11:06:08] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 11:06:08] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f39431d2310>
[03-24 11:06:08] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 11:06:08] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 11:06:08] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 11:06:08] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 11:06:08] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 11:06:08] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7f39431d2310>
[03-24 11:06:08] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 11:06:08] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 11:06:08] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 11:06:08] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 11:06:08] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=96, lbs=48, iters_train=13346, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 11:06:09] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 11:06:09] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 11:06:10] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 11:06:10] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 11:06:10] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 11:06:10] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 11:06:10] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 11:06:10] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 11:06:10] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 7.500000000000001e-05, 'weight_decay': 0}

[03-24 11:06:14] (/root/VAR/trainer.py    , line 122)=> iter: 0	loss: 8.317837715148926
[03-24 11:06:15] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [    0/13346]  eta: 19:24:17  tlr: 3.8e-07  tnm: 0.00  Lm: 8.318 (8.318)  Lt: 8.318 (8.318)  Accm: 0.03 (0.03)  Acct: 0.02 (0.02)  time: 5.2343  data: 0.5940
[03-24 11:07:43] (/root/VAR/trainer.py    , line 122)=> iter: 50	loss: 8.317489624023438
[03-24 11:09:11] (/root/VAR/trainer.py    , line 122)=> iter: 100	loss: 8.316797256469727
[03-24 11:10:39] (/root/VAR/trainer.py    , line 122)=> iter: 150	loss: 8.316095352172852
[03-24 11:12:06] (/root/VAR/trainer.py    , line 122)=> iter: 200	loss: 8.315447807312012
[03-24 11:13:33] (/root/VAR/trainer.py    , line 122)=> iter: 250	loss: 8.314851760864258
[03-24 11:15:00] (/root/VAR/trainer.py    , line 122)=> iter: 300	loss: 8.313915252685547
[03-24 11:16:28] (/root/VAR/trainer.py    , line 122)=> iter: 350	loss: 8.312212944030762
[03-24 11:17:56] (/root/VAR/trainer.py    , line 122)=> iter: 400	loss: 8.312226295471191
[03-24 11:19:25] (/root/VAR/trainer.py    , line 122)=> iter: 450	loss: 8.310916900634766
[03-24 11:19:43] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [  460/13346]  eta: 6:18:44  tlr: 1e-06  tnm: 0.00  Lm: 8.314 (8.314)  Lt: 8.311 (8.311)  Accm: 0.04 (0.04)  Acct: 0.03 (0.03)  time: 1.7726  data: 0.5763
[03-24 11:20:53] (/root/VAR/trainer.py    , line 122)=> iter: 500	loss: 8.30966567993164
[03-24 11:22:21] (/root/VAR/trainer.py    , line 122)=> iter: 550	loss: 8.308135032653809
[03-24 11:23:49] (/root/VAR/trainer.py    , line 122)=> iter: 600	loss: 8.306407928466797
[03-24 11:25:16] (/root/VAR/trainer.py    , line 122)=> iter: 650	loss: 8.30586051940918
[03-24 11:26:44] (/root/VAR/trainer.py    , line 122)=> iter: 700	loss: 8.303553581237793
[03-24 11:28:12] (/root/VAR/trainer.py    , line 122)=> iter: 750	loss: 8.301172256469727
[03-24 11:29:40] (/root/VAR/trainer.py    , line 122)=> iter: 800	loss: 8.300655364990234
[03-24 11:31:07] (/root/VAR/trainer.py    , line 122)=> iter: 850	loss: 8.297271728515625
[03-24 11:32:35] (/root/VAR/trainer.py    , line 122)=> iter: 900	loss: 8.29502010345459
[03-24 11:33:10] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [  920/13346]  eta: 6:04:24  tlr: 1.7e-06  tnm: 0.00  Lm: 8.311 (8.308)  Lt: 8.305 (8.301)  Accm: 0.05 (0.05)  Acct: 0.03 (0.03)  time: 1.7679  data: 0.5687
[03-24 11:34:03] (/root/VAR/trainer.py    , line 122)=> iter: 950	loss: 8.29569149017334
[03-24 11:35:31] (/root/VAR/trainer.py    , line 122)=> iter: 1000	loss: 8.290990829467773
[03-24 11:36:59] (/root/VAR/trainer.py    , line 122)=> iter: 1050	loss: 8.289175033569336
[03-24 11:38:26] (/root/VAR/trainer.py    , line 122)=> iter: 1100	loss: 8.283564567565918
[03-24 11:39:54] (/root/VAR/trainer.py    , line 122)=> iter: 1150	loss: 8.280600547790527
[03-24 11:41:22] (/root/VAR/trainer.py    , line 122)=> iter: 1200	loss: 8.272880554199219
[03-24 11:42:49] (/root/VAR/trainer.py    , line 122)=> iter: 1250	loss: 8.267877578735352
[03-24 11:44:17] (/root/VAR/trainer.py    , line 122)=> iter: 1300	loss: 8.262826919555664
[03-24 11:45:45] (/root/VAR/trainer.py    , line 122)=> iter: 1350	loss: 8.254692077636719
[03-24 11:46:38] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 1380/13346]  eta: 5:50:39  tlr: 2.3e-06  tnm: 0.00  Lm: 8.303 (8.295)  Lt: 8.292 (8.279)  Accm: 0.06 (0.06)  Acct: 0.04 (0.04)  time: 1.7619  data: 0.5481
[03-24 11:47:13] (/root/VAR/trainer.py    , line 122)=> iter: 1400	loss: 8.244380950927734
[03-24 11:48:41] (/root/VAR/trainer.py    , line 122)=> iter: 1450	loss: 8.23984432220459
[03-24 11:50:09] (/root/VAR/trainer.py    , line 122)=> iter: 1500	loss: 8.225052833557129
[03-24 11:51:37] (/root/VAR/trainer.py    , line 122)=> iter: 1550	loss: 8.224298477172852
[03-24 11:53:06] (/root/VAR/trainer.py    , line 122)=> iter: 1600	loss: 8.207708358764648
[03-24 11:54:34] (/root/VAR/trainer.py    , line 122)=> iter: 1650	loss: 8.190765380859375
[03-24 11:56:02] (/root/VAR/trainer.py    , line 122)=> iter: 1700	loss: 8.1837739944458
[03-24 11:57:29] (/root/VAR/trainer.py    , line 122)=> iter: 1750	loss: 8.178495407104492
[03-24 11:58:57] (/root/VAR/trainer.py    , line 122)=> iter: 1800	loss: 8.16590690612793
[03-24 12:00:08] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 1840/13346]  eta: 5:37:19  tlr: 2.9e-06  tnm: 0.00  Lm: 8.296 (8.267)  Lt: 8.280 (8.242)  Accm: 0.08 (0.08)  Acct: 0.04 (0.04)  time: 1.7601  data: 0.5618
[03-24 12:00:26] (/root/VAR/trainer.py    , line 122)=> iter: 1850	loss: 8.160181045532227
[03-24 12:01:54] (/root/VAR/trainer.py    , line 122)=> iter: 1900	loss: 8.145339965820312
[03-24 12:03:21] (/root/VAR/trainer.py    , line 122)=> iter: 1950	loss: 8.151065826416016
[03-24 12:04:50] (/root/VAR/trainer.py    , line 122)=> iter: 2000	loss: 8.120859146118164
[03-24 12:06:18] (/root/VAR/trainer.py    , line 122)=> iter: 2050	loss: 8.123906135559082
[03-24 12:07:46] (/root/VAR/trainer.py    , line 122)=> iter: 2100	loss: 8.11860466003418
[03-24 12:09:15] (/root/VAR/trainer.py    , line 122)=> iter: 2150	loss: 8.121329307556152
[03-24 12:10:44] (/root/VAR/trainer.py    , line 122)=> iter: 2200	loss: 8.117077827453613
[03-24 12:12:12] (/root/VAR/trainer.py    , line 122)=> iter: 2250	loss: 8.092784881591797
[03-24 12:13:40] (/root/VAR/trainer.py    , line 122)=> iter: 2300	loss: 8.110669136047363
[03-24 12:13:40] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 2300/13346]  eta: 5:24:03  tlr: 3.6e-06  tnm: 0.00  Lm: 8.276 (8.241)  Lt: 8.247 (8.204)  Accm: 0.09 (0.08)  Acct: 0.04 (0.05)  time: 1.7687  data: 0.5871
[03-24 12:15:08] (/root/VAR/trainer.py    , line 122)=> iter: 2350	loss: 8.092275619506836
[03-24 12:16:36] (/root/VAR/trainer.py    , line 122)=> iter: 2400	loss: 8.103208541870117
[03-24 12:18:04] (/root/VAR/trainer.py    , line 122)=> iter: 2450	loss: 8.082950592041016
[03-24 12:19:31] (/root/VAR/trainer.py    , line 122)=> iter: 2500	loss: 8.075490951538086
[03-24 12:21:00] (/root/VAR/trainer.py    , line 122)=> iter: 2550	loss: 8.084722518920898
[03-24 12:22:28] (/root/VAR/trainer.py    , line 122)=> iter: 2600	loss: 8.073351860046387
[03-24 12:23:57] (/root/VAR/trainer.py    , line 122)=> iter: 2650	loss: 8.078176498413086
[03-24 12:25:25] (/root/VAR/trainer.py    , line 122)=> iter: 2700	loss: 8.088130950927734
[03-24 12:26:53] (/root/VAR/trainer.py    , line 122)=> iter: 2750	loss: 8.07857894897461
[03-24 12:27:13] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 2761/13346]  eta: 5:10:36  tlr: 4.2e-06  tnm: 0.16  Lm: 8.256 (8.218)  Lt: 8.214 (8.180)  Accm: 0.09 (0.09)  Acct: 0.05 (0.06)  time: 1.7717  data: 0.5937
[03-24 12:28:22] (/root/VAR/trainer.py    , line 122)=> iter: 2800	loss: 8.08030891418457
[03-24 12:29:50] (/root/VAR/trainer.py    , line 122)=> iter: 2850	loss: 8.071871757507324
[03-24 12:31:18] (/root/VAR/trainer.py    , line 122)=> iter: 2900	loss: 8.054706573486328
[03-24 12:32:46] (/root/VAR/trainer.py    , line 122)=> iter: 2950	loss: 8.070714950561523
[03-24 12:34:13] (/root/VAR/trainer.py    , line 122)=> iter: 3000	loss: 8.07593822479248
[03-24 12:35:42] (/root/VAR/trainer.py    , line 122)=> iter: 3050	loss: 8.076125144958496
[03-24 12:37:10] (/root/VAR/trainer.py    , line 122)=> iter: 3100	loss: 8.04975700378418
[03-24 12:38:37] (/root/VAR/trainer.py    , line 122)=> iter: 3150	loss: 8.07533073425293
[03-24 12:40:05] (/root/VAR/trainer.py    , line 122)=> iter: 3200	loss: 8.06103801727295
[03-24 12:40:42] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 3221/13346]  eta: 4:57:05  tlr: 4.9e-06  tnm: 0.26  Lm: 8.205 (8.198)  Lt: 8.154 (8.163)  Accm: 0.09 (0.09)  Acct: 0.04 (0.05)  time: 1.7543  data: 0.5741
[03-24 12:41:33] (/root/VAR/trainer.py    , line 122)=> iter: 3250	loss: 8.062131881713867
[03-24 12:43:01] (/root/VAR/trainer.py    , line 122)=> iter: 3300	loss: 8.07297420501709
[03-24 12:44:28] (/root/VAR/trainer.py    , line 122)=> iter: 3350	loss: 8.046557426452637
[03-24 12:45:55] (/root/VAR/trainer.py    , line 122)=> iter: 3400	loss: 8.031267166137695
[03-24 12:47:23] (/root/VAR/trainer.py    , line 122)=> iter: 3450	loss: 8.058857917785645
[03-24 12:48:52] (/root/VAR/trainer.py    , line 122)=> iter: 3500	loss: 8.04530143737793
[03-24 12:50:20] (/root/VAR/trainer.py    , line 122)=> iter: 3550	loss: 8.0477876663208
[03-24 12:51:49] (/root/VAR/trainer.py    , line 122)=> iter: 3600	loss: 8.046281814575195
[03-24 12:53:16] (/root/VAR/trainer.py    , line 122)=> iter: 3650	loss: 8.05515193939209
[03-24 12:54:12] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 3681/13346]  eta: 4:43:35  tlr: 5.5e-06  tnm: 0.27  Lm: 8.154 (8.181)  Lt: 8.094 (8.148)  Accm: 0.09 (0.10)  Acct: 0.05 (0.06)  time: 1.7764  data: 0.5930
[03-24 12:54:45] (/root/VAR/trainer.py    , line 122)=> iter: 3700	loss: 8.040605545043945
[03-24 12:56:13] (/root/VAR/trainer.py    , line 122)=> iter: 3750	loss: 8.050155639648438
[03-24 12:57:41] (/root/VAR/trainer.py    , line 122)=> iter: 3800	loss: 8.054372787475586
[03-24 12:59:08] (/root/VAR/trainer.py    , line 122)=> iter: 3850	loss: 8.030031204223633
[03-24 13:00:35] (/root/VAR/trainer.py    , line 122)=> iter: 3900	loss: 8.021713256835938
[03-24 13:02:02] (/root/VAR/trainer.py    , line 122)=> iter: 3950	loss: 8.039146423339844
[03-24 13:03:31] (/root/VAR/trainer.py    , line 122)=> iter: 4000	loss: 8.043126106262207
[03-24 13:04:59] (/root/VAR/trainer.py    , line 122)=> iter: 4050	loss: 8.047688484191895
[03-24 13:06:27] (/root/VAR/trainer.py    , line 122)=> iter: 4100	loss: 8.027139663696289
[03-24 13:07:40] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 4141/13346]  eta: 4:30:00  tlr: 6.2e-06  tnm: 0.67  Lm: 8.132 (8.167)  Lt: 8.068 (8.137)  Accm: 0.09 (0.11)  Acct: 0.06 (0.06)  time: 1.7608  data: 0.5748
[03-24 13:07:55] (/root/VAR/trainer.py    , line 122)=> iter: 4150	loss: 8.04426383972168
[03-24 13:09:23] (/root/VAR/trainer.py    , line 122)=> iter: 4200	loss: 8.000995635986328
[03-24 13:11:10] (/root/VAR/trainer.py    , line 122)=> iter: 4250	loss: 8.039909362792969
[03-24 13:13:12] (/root/VAR/trainer.py    , line 122)=> iter: 4300	loss: 8.035033226013184
[03-24 13:14:40] (/root/VAR/trainer.py    , line 122)=> iter: 4350	loss: 8.023482322692871
[03-24 13:16:09] (/root/VAR/trainer.py    , line 122)=> iter: 4400	loss: 8.047117233276367
[03-24 13:17:37] (/root/VAR/trainer.py    , line 122)=> iter: 4450	loss: 8.031170845031738
[03-24 13:19:04] (/root/VAR/trainer.py    , line 122)=> iter: 4500	loss: 8.035327911376953
[03-24 13:20:33] (/root/VAR/trainer.py    , line 122)=> iter: 4550	loss: 8.036893844604492
[03-24 13:22:02] (/root/VAR/trainer.py    , line 122)=> iter: 4600	loss: 7.980449676513672
[03-24 13:22:04] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 4601/13346]  eta: 4:18:14  tlr: 6.8e-06  tnm: 0.51  Lm: 8.111 (8.155)  Lt: 8.043 (8.124)  Accm: 0.09 (0.11)  Acct: 0.07 (0.06)  time: 1.7650  data: 0.5683
[03-24 13:23:29] (/root/VAR/trainer.py    , line 122)=> iter: 4650	loss: 8.042425155639648
[03-24 13:24:57] (/root/VAR/trainer.py    , line 122)=> iter: 4700	loss: 8.011756896972656
[03-24 13:26:25] (/root/VAR/trainer.py    , line 122)=> iter: 4750	loss: 8.02074146270752
[03-24 13:27:54] (/root/VAR/trainer.py    , line 122)=> iter: 4800	loss: 8.050528526306152
[03-24 13:29:22] (/root/VAR/trainer.py    , line 122)=> iter: 4850	loss: 8.025650978088379
[03-24 13:30:50] (/root/VAR/trainer.py    , line 122)=> iter: 4900	loss: 8.030900955200195
[03-24 13:32:19] (/root/VAR/trainer.py    , line 122)=> iter: 4950	loss: 7.987579345703125
[03-24 13:33:47] (/root/VAR/trainer.py    , line 122)=> iter: 5000	loss: 7.996541500091553
[03-24 13:35:15] (/root/VAR/trainer.py    , line 122)=> iter: 5050	loss: 7.983823776245117
[03-24 13:35:35] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 5061/13346]  eta: 4:04:33  tlr: 7.4e-06  tnm: 0.76  Lm: 8.096 (8.144)  Lt: 8.041 (8.114)  Accm: 0.11 (0.11)  Acct: 0.07 (0.07)  time: 1.7642  data: 0.5673
[03-24 13:36:43] (/root/VAR/trainer.py    , line 122)=> iter: 5100	loss: 7.984192848205566
[03-24 13:38:11] (/root/VAR/trainer.py    , line 122)=> iter: 5150	loss: 8.029231071472168
[03-24 13:39:40] (/root/VAR/trainer.py    , line 122)=> iter: 5200	loss: 7.997184753417969
[03-24 13:41:08] (/root/VAR/trainer.py    , line 122)=> iter: 5250	loss: 8.029413223266602
[03-24 13:42:36] (/root/VAR/trainer.py    , line 122)=> iter: 5300	loss: 8.017904281616211
[03-24 13:44:05] (/root/VAR/trainer.py    , line 122)=> iter: 5350	loss: 8.051459312438965
[03-24 13:45:33] (/root/VAR/trainer.py    , line 122)=> iter: 5400	loss: 8.017152786254883
[03-24 13:47:01] (/root/VAR/trainer.py    , line 122)=> iter: 5450	loss: 8.016817092895508
[03-24 13:48:29] (/root/VAR/trainer.py    , line 122)=> iter: 5500	loss: 8.020540237426758
[03-24 13:49:09] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 5522/13346]  eta: 3:50:52  tlr: 8.1e-06  tnm: 0.00  Lm: 8.080 (8.133)  Lt: 8.039 (8.103)  Accm: 0.12 (0.12)  Acct: 0.07 (0.07)  time: 1.7738  data: 0.5791
[03-24 13:49:58] (/root/VAR/trainer.py    , line 122)=> iter: 5550	loss: 8.016609191894531
[03-24 13:51:25] (/root/VAR/trainer.py    , line 122)=> iter: 5600	loss: 8.007266998291016
[03-24 13:52:54] (/root/VAR/trainer.py    , line 122)=> iter: 5650	loss: 8.03564167022705
[03-24 13:54:22] (/root/VAR/trainer.py    , line 122)=> iter: 5700	loss: 7.993949890136719
[03-24 13:55:49] (/root/VAR/trainer.py    , line 122)=> iter: 5750	loss: 7.942708492279053
[03-24 13:57:18] (/root/VAR/trainer.py    , line 122)=> iter: 5800	loss: 8.004563331604004
[03-24 13:58:46] (/root/VAR/trainer.py    , line 122)=> iter: 5850	loss: 8.024499893188477
[03-24 14:00:14] (/root/VAR/trainer.py    , line 122)=> iter: 5900	loss: 8.029823303222656
[03-24 14:01:42] (/root/VAR/trainer.py    , line 122)=> iter: 5950	loss: 8.000165939331055
[03-24 14:02:40] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 5982/13346]  eta: 3:37:14  tlr: 8.7e-06  tnm: 0.00  Lm: 8.068 (8.122)  Lt: 8.036 (8.094)  Accm: 0.13 (0.14)  Acct: 0.07 (0.07)  time: 1.7950  data: 0.5903
[03-24 14:03:11] (/root/VAR/trainer.py    , line 122)=> iter: 6000	loss: 7.990024089813232
[03-24 14:04:39] (/root/VAR/trainer.py    , line 122)=> iter: 6050	loss: 8.015563011169434
[03-24 14:06:08] (/root/VAR/trainer.py    , line 122)=> iter: 6100	loss: 7.982189178466797
[03-24 14:07:36] (/root/VAR/trainer.py    , line 122)=> iter: 6150	loss: 7.993424415588379
[03-24 14:09:04] (/root/VAR/trainer.py    , line 122)=> iter: 6200	loss: 8.00216293334961
[03-24 14:10:32] (/root/VAR/trainer.py    , line 122)=> iter: 6250	loss: 7.993528366088867
[03-24 14:12:01] (/root/VAR/trainer.py    , line 122)=> iter: 6300	loss: 8.005035400390625
[03-24 14:13:29] (/root/VAR/trainer.py    , line 122)=> iter: 6350	loss: 7.996204376220703
[03-24 14:14:57] (/root/VAR/trainer.py    , line 122)=> iter: 6400	loss: 8.006707191467285
[03-24 14:16:12] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 6442/13346]  eta: 3:23:37  tlr: 9.4e-06  tnm: 0.00  Lm: 8.056 (8.115)  Lt: 8.033 (8.085)  Accm: 0.14 (0.14)  Acct: 0.07 (0.07)  time: 1.7752  data: 0.5875
[03-24 14:16:25] (/root/VAR/trainer.py    , line 122)=> iter: 6450	loss: 8.016547203063965
[03-24 14:17:53] (/root/VAR/trainer.py    , line 122)=> iter: 6500	loss: 8.017580032348633
[03-24 14:19:22] (/root/VAR/trainer.py    , line 122)=> iter: 6550	loss: 7.99896764755249
[03-24 14:20:50] (/root/VAR/trainer.py    , line 122)=> iter: 6600	loss: 8.006269454956055
[03-24 14:22:19] (/root/VAR/trainer.py    , line 122)=> iter: 6650	loss: 8.010541915893555
[03-24 14:23:47] (/root/VAR/trainer.py    , line 122)=> iter: 6700	loss: 7.999169826507568
[03-24 14:25:15] (/root/VAR/trainer.py    , line 122)=> iter: 6750	loss: 8.014717102050781
[03-24 14:26:42] (/root/VAR/trainer.py    , line 122)=> iter: 6800	loss: 7.980855464935303
[03-24 14:28:10] (/root/VAR/trainer.py    , line 122)=> iter: 6850	loss: 7.996784210205078
[03-24 14:29:39] (/root/VAR/trainer.py    , line 122)=> iter: 6900	loss: 7.974503993988037
[03-24 14:29:43] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 6902/13346]  eta: 3:10:00  tlr: 1e-05  tnm: 0.00  Lm: 8.053 (8.109)  Lt: 8.029 (8.078)  Accm: 0.14 (0.14)  Acct: 0.07 (0.07)  time: 1.7788  data: 0.5755
[03-24 14:31:07] (/root/VAR/trainer.py    , line 122)=> iter: 6950	loss: 7.9656476974487305
[03-24 14:32:35] (/root/VAR/trainer.py    , line 122)=> iter: 7000	loss: 7.980584144592285
[03-24 14:34:03] (/root/VAR/trainer.py    , line 122)=> iter: 7050	loss: 8.002361297607422
[03-24 14:35:31] (/root/VAR/trainer.py    , line 122)=> iter: 7100	loss: 8.027772903442383
[03-24 14:36:59] (/root/VAR/trainer.py    , line 122)=> iter: 7150	loss: 7.914346218109131
[03-24 14:38:27] (/root/VAR/trainer.py    , line 122)=> iter: 7200	loss: 7.988146781921387
[03-24 14:39:55] (/root/VAR/trainer.py    , line 122)=> iter: 7250	loss: 8.011177062988281
[03-24 14:41:23] (/root/VAR/trainer.py    , line 122)=> iter: 7300	loss: 7.961271286010742
[03-24 14:42:51] (/root/VAR/trainer.py    , line 122)=> iter: 7350	loss: 7.987208366394043
[03-24 14:43:13] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 7362/13346]  eta: 2:56:24  tlr: 1.1e-05  tnm: 0.00  Lm: 8.050 (8.101)  Lt: 8.025 (8.071)  Accm: 0.14 (0.15)  Acct: 0.07 (0.07)  time: 1.7850  data: 0.5650
[03-24 14:44:20] (/root/VAR/trainer.py    , line 122)=> iter: 7400	loss: 8.028833389282227
[03-24 14:45:47] (/root/VAR/trainer.py    , line 122)=> iter: 7450	loss: 7.9740309715271
[03-24 14:47:15] (/root/VAR/trainer.py    , line 122)=> iter: 7500	loss: 8.005590438842773
[03-24 14:48:43] (/root/VAR/trainer.py    , line 122)=> iter: 7550	loss: 8.010992050170898
[03-24 14:50:11] (/root/VAR/trainer.py    , line 122)=> iter: 7600	loss: 8.01042366027832
[03-24 14:51:40] (/root/VAR/trainer.py    , line 122)=> iter: 7650	loss: 7.956881523132324
[03-24 14:53:07] (/root/VAR/trainer.py    , line 122)=> iter: 7700	loss: 7.986876964569092
[03-24 14:54:35] (/root/VAR/trainer.py    , line 122)=> iter: 7750	loss: 8.003765106201172
[03-24 14:56:04] (/root/VAR/trainer.py    , line 122)=> iter: 7800	loss: 7.995877265930176
[03-24 14:56:43] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 7822/13346]  eta: 2:42:47  tlr: 1.1e-05  tnm: 0.00  Lm: 8.045 (8.096)  Lt: 8.021 (8.064)  Accm: 0.14 (0.15)  Acct: 0.07 (0.08)  time: 1.7710  data: 0.5705
[03-24 14:57:32] (/root/VAR/trainer.py    , line 122)=> iter: 7850	loss: 7.954390525817871
[03-24 14:59:00] (/root/VAR/trainer.py    , line 122)=> iter: 7900	loss: 8.018569946289062
[03-24 15:00:28] (/root/VAR/trainer.py    , line 122)=> iter: 7950	loss: 7.974181652069092
[03-24 15:01:56] (/root/VAR/trainer.py    , line 122)=> iter: 8000	loss: 7.9587531089782715
[03-24 15:03:24] (/root/VAR/trainer.py    , line 122)=> iter: 8050	loss: 7.987940788269043
[03-24 15:04:53] (/root/VAR/trainer.py    , line 122)=> iter: 8100	loss: 7.972126007080078
[03-24 15:06:21] (/root/VAR/trainer.py    , line 122)=> iter: 8150	loss: 7.962787628173828
[03-24 15:07:49] (/root/VAR/trainer.py    , line 122)=> iter: 8200	loss: 8.006452560424805
[03-24 15:09:17] (/root/VAR/trainer.py    , line 122)=> iter: 8250	loss: 7.984307765960693
[03-24 15:10:16] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 8283/13346]  eta: 2:29:11  tlr: 1.2e-05  tnm: 0.40  Lm: 8.039 (8.092)  Lt: 8.016 (8.059)  Accm: 0.14 (0.15)  Acct: 0.07 (0.08)  time: 1.7590  data: 0.5830
[03-24 15:10:45] (/root/VAR/trainer.py    , line 122)=> iter: 8300	loss: 7.9819655418396
[03-24 15:12:14] (/root/VAR/trainer.py    , line 122)=> iter: 8350	loss: 7.993617534637451
[03-24 15:13:42] (/root/VAR/trainer.py    , line 122)=> iter: 8400	loss: 7.9964094161987305
[03-24 15:15:11] (/root/VAR/trainer.py    , line 122)=> iter: 8450	loss: 7.986414909362793
[03-24 15:16:39] (/root/VAR/trainer.py    , line 122)=> iter: 8500	loss: 7.941397666931152
[03-24 15:18:07] (/root/VAR/trainer.py    , line 122)=> iter: 8550	loss: 8.006465911865234
[03-24 15:19:35] (/root/VAR/trainer.py    , line 122)=> iter: 8600	loss: 7.954938888549805
[03-24 15:21:03] (/root/VAR/trainer.py    , line 122)=> iter: 8650	loss: 7.981725215911865
[03-24 15:22:31] (/root/VAR/trainer.py    , line 122)=> iter: 8700	loss: 7.931292533874512
[03-24 15:23:48] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 8743/13346]  eta: 2:15:37  tlr: 1.3e-05  tnm: 0.77  Lm: 8.037 (8.085)  Lt: 8.009 (8.053)  Accm: 0.14 (0.15)  Acct: 0.08 (0.08)  time: 1.7597  data: 0.5872
[03-24 15:24:00] (/root/VAR/trainer.py    , line 122)=> iter: 8750	loss: 7.967266082763672
[03-24 15:25:27] (/root/VAR/trainer.py    , line 122)=> iter: 8800	loss: 7.913145065307617
[03-24 15:26:56] (/root/VAR/trainer.py    , line 122)=> iter: 8850	loss: 7.9352545738220215
[03-24 15:28:23] (/root/VAR/trainer.py    , line 122)=> iter: 8900	loss: 7.9806108474731445
[03-24 15:29:51] (/root/VAR/trainer.py    , line 122)=> iter: 8950	loss: 7.943254470825195
[03-24 15:31:19] (/root/VAR/trainer.py    , line 122)=> iter: 9000	loss: 7.997949123382568
[03-24 15:32:47] (/root/VAR/trainer.py    , line 122)=> iter: 9050	loss: 8.00178337097168
[03-24 15:34:15] (/root/VAR/trainer.py    , line 122)=> iter: 9100	loss: 7.94556188583374
[03-24 15:35:43] (/root/VAR/trainer.py    , line 122)=> iter: 9150	loss: 7.979290008544922
[03-24 15:37:11] (/root/VAR/trainer.py    , line 122)=> iter: 9200	loss: 7.964118957519531
[03-24 15:37:17] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 9203/13346]  eta: 2:02:02  tlr: 1.3e-05  tnm: 0.32  Lm: 8.036 (8.077)  Lt: 8.001 (8.048)  Accm: 0.15 (0.16)  Acct: 0.08 (0.08)  time: 1.7600  data: 0.5599
[03-24 15:38:40] (/root/VAR/trainer.py    , line 122)=> iter: 9250	loss: 7.940817832946777
[03-24 15:40:08] (/root/VAR/trainer.py    , line 122)=> iter: 9300	loss: 7.923863887786865
[03-24 15:41:37] (/root/VAR/trainer.py    , line 122)=> iter: 9350	loss: 7.931059837341309
[03-24 15:43:04] (/root/VAR/trainer.py    , line 122)=> iter: 9400	loss: 7.964215278625488
[03-24 15:44:33] (/root/VAR/trainer.py    , line 122)=> iter: 9450	loss: 7.98614501953125
[03-24 15:46:00] (/root/VAR/trainer.py    , line 122)=> iter: 9500	loss: 7.998767852783203
[03-24 15:47:29] (/root/VAR/trainer.py    , line 122)=> iter: 9550	loss: 7.951704978942871
[03-24 15:48:56] (/root/VAR/trainer.py    , line 122)=> iter: 9600	loss: 8.00837516784668
[03-24 15:50:25] (/root/VAR/trainer.py    , line 122)=> iter: 9650	loss: 7.982134819030762
[03-24 15:50:49] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [ 9663/13346]  eta: 1:48:28  tlr: 1.4e-05  tnm: 0.31  Lm: 8.027 (8.073)  Lt: 7.998 (8.044)  Accm: 0.16 (0.16)  Acct: 0.08 (0.09)  time: 1.7853  data: 0.6039
[03-24 15:51:53] (/root/VAR/trainer.py    , line 122)=> iter: 9700	loss: 7.90274715423584
[03-24 15:53:21] (/root/VAR/trainer.py    , line 122)=> iter: 9750	loss: 7.9443678855896
[03-24 15:54:50] (/root/VAR/trainer.py    , line 122)=> iter: 9800	loss: 7.974042892456055
[03-24 15:56:18] (/root/VAR/trainer.py    , line 122)=> iter: 9850	loss: 8.00200080871582
[03-24 15:57:46] (/root/VAR/trainer.py    , line 122)=> iter: 9900	loss: 7.934288024902344
[03-24 15:59:15] (/root/VAR/trainer.py    , line 122)=> iter: 9950	loss: 7.992997646331787
[03-24 16:00:43] (/root/VAR/trainer.py    , line 122)=> iter: 10000	loss: 7.9808783531188965
[03-24 16:02:11] (/root/VAR/trainer.py    , line 122)=> iter: 10050	loss: 7.9731903076171875
[03-24 16:03:39] (/root/VAR/trainer.py    , line 122)=> iter: 10100	loss: 7.975766658782959
[03-24 16:04:20] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [10123/13346]  eta: 1:34:55  tlr: 1.5e-05  tnm: 0.32  Lm: 8.019 (8.068)  Lt: 7.996 (8.040)  Accm: 0.17 (0.17)  Acct: 0.08 (0.09)  time: 1.7681  data: 0.5632
[03-24 16:05:07] (/root/VAR/trainer.py    , line 122)=> iter: 10150	loss: 7.928617000579834
[03-24 16:06:35] (/root/VAR/trainer.py    , line 122)=> iter: 10200	loss: 7.928863525390625
[03-24 16:08:03] (/root/VAR/trainer.py    , line 122)=> iter: 10250	loss: 7.958325386047363
[03-24 16:09:31] (/root/VAR/trainer.py    , line 122)=> iter: 10300	loss: 7.966594219207764
[03-24 16:10:59] (/root/VAR/trainer.py    , line 122)=> iter: 10350	loss: 7.958455562591553
[03-24 16:12:27] (/root/VAR/trainer.py    , line 122)=> iter: 10400	loss: 7.9438796043396
[03-24 16:13:55] (/root/VAR/trainer.py    , line 122)=> iter: 10450	loss: 7.974473476409912
[03-24 16:15:23] (/root/VAR/trainer.py    , line 122)=> iter: 10500	loss: 7.932353973388672
[03-24 16:16:50] (/root/VAR/trainer.py    , line 122)=> iter: 10550	loss: 7.97053861618042
[03-24 16:17:49] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [10583/13346]  eta: 1:21:21  tlr: 1.5e-05  tnm: 0.59  Lm: 8.018 (8.064)  Lt: 7.988 (8.036)  Accm: 0.17 (0.17)  Acct: 0.09 (0.09)  time: 1.7648  data: 0.5671
[03-24 16:18:18] (/root/VAR/trainer.py    , line 122)=> iter: 10600	loss: 7.911269187927246
[03-24 16:19:47] (/root/VAR/trainer.py    , line 122)=> iter: 10650	loss: 7.990397930145264
[03-24 16:21:15] (/root/VAR/trainer.py    , line 122)=> iter: 10700	loss: 7.9656805992126465
[03-24 16:22:44] (/root/VAR/trainer.py    , line 122)=> iter: 10750	loss: 7.93881893157959
[03-24 16:24:12] (/root/VAR/trainer.py    , line 122)=> iter: 10800	loss: 7.926614284515381
[03-24 16:25:40] (/root/VAR/trainer.py    , line 122)=> iter: 10850	loss: 7.936803340911865
[03-24 16:27:09] (/root/VAR/trainer.py    , line 122)=> iter: 10900	loss: 7.96280574798584
[03-24 16:28:37] (/root/VAR/trainer.py    , line 122)=> iter: 10950	loss: 7.943343162536621
[03-24 16:30:06] (/root/VAR/trainer.py    , line 122)=> iter: 11000	loss: 7.994353294372559
[03-24 16:31:24] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [11044/13346]  eta: 1:07:47  tlr: 1.6e-05  tnm: 0.00  Lm: 8.018 (8.060)  Lt: 7.981 (8.031)  Accm: 0.18 (0.17)  Acct: 0.09 (0.09)  time: 1.7769  data: 0.5795
[03-24 16:31:34] (/root/VAR/trainer.py    , line 122)=> iter: 11050	loss: 7.969598293304443
[03-24 16:33:03] (/root/VAR/trainer.py    , line 122)=> iter: 11100	loss: 7.946811199188232
[03-24 16:34:31] (/root/VAR/trainer.py    , line 122)=> iter: 11150	loss: 8.001640319824219
[03-24 16:35:59] (/root/VAR/trainer.py    , line 122)=> iter: 11200	loss: 7.970651626586914
[03-24 16:37:27] (/root/VAR/trainer.py    , line 122)=> iter: 11250	loss: 7.979955196380615
[03-24 16:38:55] (/root/VAR/trainer.py    , line 122)=> iter: 11300	loss: 7.993300914764404
[03-24 16:40:22] (/root/VAR/trainer.py    , line 122)=> iter: 11350	loss: 7.986065864562988
[03-24 16:41:45] (/root/VAR/trainer.py    , line 122)=> iter: 11400	loss: 7.956239223480225
[03-24 16:43:08] (/root/VAR/trainer.py    , line 122)=> iter: 11450	loss: 7.968125343322754
[03-24 16:44:30] (/root/VAR/trainer.py    , line 122)=> iter: 11500	loss: 7.968362331390381
[03-24 16:44:37] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [11504/13346]  eta: 0:54:11  tlr: 1.6e-05  tnm: 0.00  Lm: 8.017 (8.055)  Lt: 7.975 (8.028)  Accm: 0.18 (0.18)  Acct: 0.09 (0.09)  time: 1.6399  data: 0.4451
[03-24 16:45:52] (/root/VAR/trainer.py    , line 122)=> iter: 11550	loss: 7.9610137939453125
[03-24 16:47:14] (/root/VAR/trainer.py    , line 122)=> iter: 11600	loss: 7.905333518981934
[03-24 16:48:36] (/root/VAR/trainer.py    , line 122)=> iter: 11650	loss: 7.930871486663818
[03-24 16:49:57] (/root/VAR/trainer.py    , line 122)=> iter: 11700	loss: 7.930427551269531
[03-24 16:51:19] (/root/VAR/trainer.py    , line 122)=> iter: 11750	loss: 7.947345733642578
[03-24 16:52:41] (/root/VAR/trainer.py    , line 122)=> iter: 11800	loss: 7.90194845199585
[03-24 16:54:03] (/root/VAR/trainer.py    , line 122)=> iter: 11850	loss: 7.960822105407715
[03-24 16:55:25] (/root/VAR/trainer.py    , line 122)=> iter: 11900	loss: 7.9661078453063965
[03-24 16:56:46] (/root/VAR/trainer.py    , line 122)=> iter: 11950	loss: 7.904210567474365
[03-24 16:57:10] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [11964/13346]  eta: 0:40:32  tlr: 1.7e-05  tnm: 0.00  Lm: 8.015 (8.051)  Lt: 7.968 (8.023)  Accm: 0.18 (0.18)  Acct: 0.09 (0.10)  time: 1.6423  data: 0.4492
[03-24 16:58:08] (/root/VAR/trainer.py    , line 122)=> iter: 12000	loss: 7.943030834197998
[03-24 16:59:30] (/root/VAR/trainer.py    , line 122)=> iter: 12050	loss: 7.957252025604248
[03-24 17:00:52] (/root/VAR/trainer.py    , line 122)=> iter: 12100	loss: 7.949326992034912
[03-24 17:02:14] (/root/VAR/trainer.py    , line 122)=> iter: 12150	loss: 7.970866680145264
[03-24 17:03:35] (/root/VAR/trainer.py    , line 122)=> iter: 12200	loss: 7.94149923324585
[03-24 17:04:58] (/root/VAR/trainer.py    , line 122)=> iter: 12250	loss: 7.913784027099609
[03-24 17:06:20] (/root/VAR/trainer.py    , line 122)=> iter: 12300	loss: 7.940459728240967
[03-24 17:07:42] (/root/VAR/trainer.py    , line 122)=> iter: 12350	loss: 7.932864189147949
[03-24 17:09:02] (/root/VAR/trainer.py    , line 122)=> iter: 12400	loss: 7.90761137008667
[03-24 17:09:42] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [12424/13346]  eta: 0:26:58  tlr: 1.8e-05  tnm: 0.00  Lm: 8.013 (8.047)  Lt: 7.966 (8.019)  Accm: 0.18 (0.19)  Acct: 0.09 (0.10)  time: 1.6170  data: 0.4244
[03-24 17:10:23] (/root/VAR/trainer.py    , line 122)=> iter: 12450	loss: 7.969717979431152
[03-24 17:11:44] (/root/VAR/trainer.py    , line 122)=> iter: 12500	loss: 7.934271812438965
[03-24 17:13:05] (/root/VAR/trainer.py    , line 122)=> iter: 12550	loss: 7.9776716232299805
[03-24 17:14:25] (/root/VAR/trainer.py    , line 122)=> iter: 12600	loss: 7.893073081970215
[03-24 17:15:45] (/root/VAR/trainer.py    , line 122)=> iter: 12650	loss: 7.947377681732178
[03-24 17:17:06] (/root/VAR/trainer.py    , line 122)=> iter: 12700	loss: 7.930106163024902
[03-24 17:18:26] (/root/VAR/trainer.py    , line 122)=> iter: 12750	loss: 7.964869499206543
[03-24 17:19:47] (/root/VAR/trainer.py    , line 122)=> iter: 12800	loss: 7.936068534851074
[03-24 17:21:07] (/root/VAR/trainer.py    , line 122)=> iter: 12850	loss: 7.914397239685059
[03-24 17:22:03] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [12884/13346]  eta: 0:13:28  tlr: 1.8e-05  tnm: 0.00  Lm: 8.011 (8.043)  Lt: 7.964 (8.016)  Accm: 0.18 (0.19)  Acct: 0.10 (0.10)  time: 1.6389  data: 0.4298
[03-24 17:22:28] (/root/VAR/trainer.py    , line 122)=> iter: 12900	loss: 7.884023189544678
[03-24 17:23:49] (/root/VAR/trainer.py    , line 122)=> iter: 12950	loss: 7.930333137512207
[03-24 17:25:09] (/root/VAR/trainer.py    , line 122)=> iter: 13000	loss: 7.936456680297852
[03-24 17:26:30] (/root/VAR/trainer.py    , line 122)=> iter: 13050	loss: 7.904971122741699
[03-24 17:27:50] (/root/VAR/trainer.py    , line 122)=> iter: 13100	loss: 7.939472198486328
[03-24 17:29:11] (/root/VAR/trainer.py    , line 122)=> iter: 13150	loss: 7.874420166015625
[03-24 17:30:32] (/root/VAR/trainer.py    , line 122)=> iter: 13200	loss: 7.912343502044678
[03-24 17:31:52] (/root/VAR/trainer.py    , line 122)=> iter: 13250	loss: 7.926145076751709
[03-24 17:33:13] (/root/VAR/trainer.py    , line 122)=> iter: 13300	loss: 7.926042556762695
[03-24 17:34:26] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   0/200]  [13345/13346]  eta: 0:00:01  tlr: 1.9e-05  tnm: 0.39  Lm: 8.009 (8.040)  Lt: 7.964 (8.013)  Accm: 0.19 (0.19)  Acct: 0.11 (0.10)  time: 1.6222  data: 0.4246
[03-24 17:34:26] (/root/VAR/utils/misc.py , line 336)=> [Ep]: [   0/200]   Total time:      6:28:16   (1.746 s / it)
[03-24 17:34:26] (/root/VAR/train.py      , line 233)=>      [ep0]  (training )  Lm: 8.039 (8.039), Lt: 8.014 (8.014),  Acc m&t: 0.20 0.11,  Remain: 49 days, 15:38:49,  Finish: 2025-05-13 09:13
[03-24 17:34:27] (/root/VAR/trainer.py    , line 122)=> iter: 0	loss: 7.931325912475586
[03-24 17:34:28] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   1/200]  [    0/13346]  eta: 6:35:15  tlr: 1.9e-05  tnm: 0.00  Lm: 7.931 (7.931)  Lt: 7.917 (7.917)  Accm: 0.31 (0.31)  Acct: 0.16 (0.16)  time: 1.7770  data: 0.4667
[03-24 17:35:47] (/root/VAR/trainer.py    , line 122)=> iter: 50	loss: 7.937930107116699
[03-24 17:37:07] (/root/VAR/trainer.py    , line 122)=> iter: 100	loss: 7.882606506347656
[03-24 17:38:28] (/root/VAR/trainer.py    , line 122)=> iter: 150	loss: 7.8961687088012695
[03-24 17:39:48] (/root/VAR/trainer.py    , line 122)=> iter: 200	loss: 7.93552303314209
[03-24 17:41:08] (/root/VAR/trainer.py    , line 122)=> iter: 250	loss: 7.930611610412598
[03-24 17:42:28] (/root/VAR/trainer.py    , line 122)=> iter: 300	loss: 7.911609649658203
[03-24 17:43:48] (/root/VAR/trainer.py    , line 122)=> iter: 350	loss: 7.882761001586914
[03-24 17:45:08] (/root/VAR/trainer.py    , line 122)=> iter: 400	loss: 7.913270473480225
[03-24 17:46:28] (/root/VAR/trainer.py    , line 122)=> iter: 450	loss: 7.920287132263184
[03-24 17:46:45] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   1/200]  [  460/13346]  eta: 5:44:24  tlr: 2e-05  tnm: 0.00  Lm: 7.909 (7.909)  Lt: 7.910 (7.910)  Accm: 0.39 (0.39)  Acct: 0.19 (0.19)  time: 1.6214  data: 0.4239
[03-24 17:47:49] (/root/VAR/trainer.py    , line 122)=> iter: 500	loss: 7.909000396728516
[03-24 17:49:09] (/root/VAR/trainer.py    , line 122)=> iter: 550	loss: 7.874696254730225
[03-24 17:50:29] (/root/VAR/trainer.py    , line 122)=> iter: 600	loss: 7.971608638763428
[03-24 17:51:50] (/root/VAR/trainer.py    , line 122)=> iter: 650	loss: 7.900144577026367
[03-24 17:53:10] (/root/VAR/trainer.py    , line 122)=> iter: 700	loss: 7.879055976867676
[03-24 17:54:29] (/root/VAR/trainer.py    , line 122)=> iter: 750	loss: 7.900768280029297
[03-24 17:55:49] (/root/VAR/trainer.py    , line 122)=> iter: 800	loss: 7.926934719085693
[03-24 17:57:09] (/root/VAR/trainer.py    , line 122)=> iter: 850	loss: 7.929381370544434
[03-24 17:58:29] (/root/VAR/trainer.py    , line 122)=> iter: 900	loss: 7.876178741455078
[03-24 17:59:02] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   1/200]  [  920/13346]  eta: 5:31:57  tlr: 2e-05  tnm: 0.00  Lm: 7.890 (7.903)  Lt: 7.906 (7.909)  Accm: 0.39 (0.39)  Acct: 0.22 (0.20)  time: 1.6183  data: 0.4162
[03-24 17:59:50] (/root/VAR/trainer.py    , line 122)=> iter: 950	loss: 7.904426574707031
[03-24 18:01:10] (/root/VAR/trainer.py    , line 122)=> iter: 1000	loss: 7.945965766906738
[03-24 18:02:31] (/root/VAR/trainer.py    , line 122)=> iter: 1050	loss: 7.8924665451049805
[03-24 18:03:51] (/root/VAR/trainer.py    , line 122)=> iter: 1100	loss: 7.8808913230896
[03-24 18:05:11] (/root/VAR/trainer.py    , line 122)=> iter: 1150	loss: 7.893630027770996
[03-24 18:06:32] (/root/VAR/trainer.py    , line 122)=> iter: 1200	loss: 7.940841197967529
[03-24 18:07:52] (/root/VAR/trainer.py    , line 122)=> iter: 1250	loss: 7.980043411254883
[03-24 18:09:12] (/root/VAR/trainer.py    , line 122)=> iter: 1300	loss: 7.883537292480469
[03-24 18:10:32] (/root/VAR/trainer.py    , line 122)=> iter: 1350	loss: 7.891854763031006
[03-24 18:11:21] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   1/200]  [ 1380/13346]  eta: 5:19:49  tlr: 2.1e-05  tnm: 0.00  Lm: 7.888 (7.895)  Lt: 7.905 (7.896)  Accm: 0.43 (0.44)  Acct: 0.22 (0.26)  time: 1.6202  data: 0.4287
[03-24 18:11:53] (/root/VAR/trainer.py    , line 122)=> iter: 1400	loss: 7.925771713256836
[03-24 18:13:13] (/root/VAR/trainer.py    , line 122)=> iter: 1450	loss: 7.915562629699707
[03-24 18:14:33] (/root/VAR/trainer.py    , line 122)=> iter: 1500	loss: 7.925320625305176
[03-24 18:15:53] (/root/VAR/trainer.py    , line 122)=> iter: 1550	loss: 7.809945583343506
[03-24 18:17:13] (/root/VAR/trainer.py    , line 122)=> iter: 1600	loss: 7.926841735839844
[03-24 18:18:33] (/root/VAR/trainer.py    , line 122)=> iter: 1650	loss: 7.883870601654053
[03-24 18:19:54] (/root/VAR/trainer.py    , line 122)=> iter: 1700	loss: 7.9442291259765625
[03-24 18:21:14] (/root/VAR/trainer.py    , line 122)=> iter: 1750	loss: 7.869297981262207
[03-24 18:22:34] (/root/VAR/trainer.py    , line 122)=> iter: 1800	loss: 7.887078285217285
[03-24 18:23:38] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   1/200]  [ 1840/13346]  eta: 5:07:30  tlr: 2.2e-05  tnm: 0.00  Lm: 7.890 (7.896)  Lt: 7.906 (7.898)  Accm: 0.39 (0.41)  Acct: 0.22 (0.25)  time: 1.6080  data: 0.4210
[03-24 18:23:54] (/root/VAR/trainer.py    , line 122)=> iter: 1850	loss: 7.886203289031982
[03-24 18:25:14] (/root/VAR/trainer.py    , line 122)=> iter: 1900	loss: 7.902987957000732
[03-24 18:26:34] (/root/VAR/trainer.py    , line 122)=> iter: 1950	loss: 7.82867956161499
[03-24 18:27:54] (/root/VAR/trainer.py    , line 122)=> iter: 2000	loss: 7.8995890617370605
[03-24 18:29:14] (/root/VAR/trainer.py    , line 122)=> iter: 2050	loss: 7.8657331466674805
[03-24 18:30:34] (/root/VAR/trainer.py    , line 122)=> iter: 2100	loss: 7.851140975952148
[03-24 18:31:55] (/root/VAR/trainer.py    , line 122)=> iter: 2150	loss: 7.880644798278809
[03-24 18:33:16] (/root/VAR/trainer.py    , line 122)=> iter: 2200	loss: 7.914924621582031
[03-24 18:34:37] (/root/VAR/trainer.py    , line 122)=> iter: 2250	loss: 7.9287261962890625
[03-24 18:35:57] (/root/VAR/trainer.py    , line 122)=> iter: 2300	loss: 7.814673900604248
[03-24 18:35:58] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   1/200]  [ 2300/13346]  eta: 4:55:23  tlr: 2.2e-05  tnm: 0.00  Lm: 7.888 (7.883)  Lt: 7.905 (7.894)  Accm: 0.43 (0.46)  Acct: 0.22 (0.26)  time: 1.6343  data: 0.4366
[03-24 18:37:18] (/root/VAR/trainer.py    , line 122)=> iter: 2350	loss: 7.902853965759277
[03-24 18:38:39] (/root/VAR/trainer.py    , line 122)=> iter: 2400	loss: 7.954723358154297
[03-24 18:40:00] (/root/VAR/trainer.py    , line 122)=> iter: 2450	loss: 7.839023590087891
[03-24 18:41:20] (/root/VAR/trainer.py    , line 122)=> iter: 2500	loss: 7.914485454559326
[03-24 18:42:41] (/root/VAR/trainer.py    , line 122)=> iter: 2550	loss: 7.892345428466797
[03-24 18:44:01] (/root/VAR/trainer.py    , line 122)=> iter: 2600	loss: 7.901244640350342
[03-24 18:45:22] (/root/VAR/trainer.py    , line 122)=> iter: 2650	loss: 7.955809593200684
[03-24 18:46:43] (/root/VAR/trainer.py    , line 122)=> iter: 2700	loss: 7.885478973388672
[03-24 18:48:03] (/root/VAR/trainer.py    , line 122)=> iter: 2750	loss: 7.8962602615356445
[03-24 18:48:21] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   1/200]  [ 2761/13346]  eta: 4:43:17  tlr: 2.3e-05  tnm: 0.25  Lm: 7.887 (7.882)  Lt: 7.903 (7.891)  Accm: 0.46 (0.46)  Acct: 0.23 (0.26)  time: 1.6056  data: 0.4138
[03-24 18:49:23] (/root/VAR/trainer.py    , line 122)=> iter: 2800	loss: 7.90643310546875
[03-24 18:50:44] (/root/VAR/trainer.py    , line 122)=> iter: 2850	loss: 7.890812873840332
[03-24 18:52:05] (/root/VAR/trainer.py    , line 122)=> iter: 2900	loss: 7.913228511810303
[03-24 18:53:26] (/root/VAR/trainer.py    , line 122)=> iter: 2950	loss: 7.822208404541016
[03-24 18:54:47] (/root/VAR/trainer.py    , line 122)=> iter: 3000	loss: 7.924415588378906
[03-24 18:56:08] (/root/VAR/trainer.py    , line 122)=> iter: 3050	loss: 7.805113792419434
[03-24 18:57:28] (/root/VAR/trainer.py    , line 122)=> iter: 3100	loss: 7.841734886169434
[03-24 18:58:49] (/root/VAR/trainer.py    , line 122)=> iter: 3150	loss: 7.859168529510498
[03-24 19:00:09] (/root/VAR/trainer.py    , line 122)=> iter: 3200	loss: 7.896745204925537
[03-24 19:00:44] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   1/200]  [ 3221/13346]  eta: 4:31:10  tlr: 2.4e-05  tnm: 0.27  Lm: 7.882 (7.879)  Lt: 7.892 (7.890)  Accm: 0.47 (0.47)  Acct: 0.23 (0.26)  time: 1.6085  data: 0.4358
[03-24 19:01:30] (/root/VAR/trainer.py    , line 122)=> iter: 3250	loss: 7.8589067459106445
[03-24 19:02:50] (/root/VAR/trainer.py    , line 122)=> iter: 3300	loss: 7.892271995544434
[03-24 19:04:10] (/root/VAR/trainer.py    , line 122)=> iter: 3350	loss: 7.897714614868164
[03-24 19:05:37] (/root/VAR/trainer.py    , line 122)=> iter: 3400	loss: 7.828317165374756
[03-24 19:07:02] (/root/VAR/trainer.py    , line 122)=> iter: 3450	loss: 7.9020538330078125
[03-24 19:08:28] (/root/VAR/trainer.py    , line 122)=> iter: 3500	loss: 7.8575239181518555
[03-24 19:09:53] (/root/VAR/trainer.py    , line 122)=> iter: 3550	loss: 7.885444641113281
[03-24 19:11:19] (/root/VAR/trainer.py    , line 122)=> iter: 3600	loss: 7.783723831176758
[03-24 19:12:45] (/root/VAR/trainer.py    , line 122)=> iter: 3650	loss: 7.871488094329834
[03-24 19:13:39] (/root/VAR/utils/misc.py , line 314)=> [Ep]: [   1/200]  [ 3681/13346]  eta: 4:20:24  tlr: 2.4e-05  tnm: 0.26  Lm: 7.877 (7.867)  Lt: 7.880 (7.883)  Accm: 0.48 (0.50)  Acct: 0.23 (0.28)  time: 1.7167  data: 0.5170
[03-24 19:14:11] (/root/VAR/trainer.py    , line 122)=> iter: 3700	loss: 7.881887435913086
[03-24 19:15:36] (/root/VAR/trainer.py    , line 122)=> iter: 3750	loss: 7.897617340087891
[03-24 19:17:01] (/root/VAR/trainer.py    , line 122)=> iter: 3800	loss: 7.824618339538574







=======================================================   RESTART [03-24 19:20:38]   =======================================================
[03-24 19:20:38] (ot/VAR/utils/arg_util.py, line 173)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[03-24 19:20:38] (ot/VAR/utils/arg_util.py, line 174)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[03-24 19:20:38] (ot/VAR/utils/arg_util.py, line 175)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[03-24 19:20:38] (/root/VAR/train.py      , line  36)=> global bs=768, local bs=768
[03-24 19:20:38] (/root/VAR/train.py      , line  37)=> initial args:
{
  data_path           : /root/autodl-tmp/downloads
  exp_name            : text
  vfast               : 0
  tfast               : 0
  depth               : 16
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 0.00030000000000000003
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  bs                  : 768
  batch_size          : 768
  glb_batch_size      : 768
  ac                  : 1
  ep                  : 200
  wp                  : 4.0
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.6666666666666666
  cmd                 : --depth=16 --bs=768 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1
  branch              : main
  commit_id           : 3f1029565306d1b471643e7c360294c8a0b2a8e4
  commit_msg          : [upd] Third-party Usage and Research
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /root/VAR/local_output
  tb_log_dir_path     : /root/VAR/local_output/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b768ep200adamlr0.0001wd0.05
  log_txt_path        : /root/VAR/local_output/log.txt
  last_ckpt_path      : /root/VAR/local_output/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[03-24 19:20:38] (/root/VAR/train.py      , line  41)=> [build PT data] ...

[03-24 19:20:41] (/root/VAR/utils/data.py , line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[03-24 19:20:41] (/root/VAR/utils/data.py , line  48)=> Transform [train] = 
[03-24 19:20:41] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 19:20:41] (/root/VAR/utils/data.py , line  51)=> RandomCrop(size=(256, 256), padding=None)
[03-24 19:20:41] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 19:20:41] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7fac4ffe0310>
[03-24 19:20:41] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 19:20:41] (/root/VAR/utils/data.py , line  48)=> Transform [val] = 
[03-24 19:20:41] (/root/VAR/utils/data.py , line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[03-24 19:20:41] (/root/VAR/utils/data.py , line  51)=> CenterCrop(size=(256, 256))
[03-24 19:20:41] (/root/VAR/utils/data.py , line  51)=> ToTensor()
[03-24 19:20:41] (/root/VAR/utils/data.py , line  51)=> <function normalize_01_into_pm1 at 0x7fac4ffe0310>
[03-24 19:20:41] (/root/VAR/utils/data.py , line  54)=> ---------------------------

[03-24 19:20:41] (/root/VAR/train.py      , line  64)=> [auto_resume] no ckpt found @ /root/VAR/local_output/ar-ckpt*.pth
[03-24 19:20:41] (/root/VAR/train.py      , line  64)=> [auto_resume quit]
[03-24 19:20:41] (/root/VAR/train.py      , line  65)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[03-24 19:20:41] (/root/VAR/train.py      , line  71)=> [dataloader] gbs=768, lbs=768, iters_train=1669, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[03-24 19:20:42] (/root/VAR/models/var.py , line  98)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[03-24 19:20:42] (/root/VAR/models/var.py , line 239)=> [init_weights] VAR with init_std=0.0180422
[03-24 19:20:43] (/root/VAR/train.py      , line 104)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[03-24 19:20:43] (/root/VAR/train.py      , line 106)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[03-24 19:20:43] (/root/VAR/train.py      , line 107)=> [INIT][#para] VAR=310.28


[03-24 19:20:43] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[03-24 19:20:43] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=310283520
[03-24 19:20:43] (/VAR/utils/lr_control.py, line 105)=> 
[03-24 19:20:43] (/root/VAR/train.py      , line 122)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 0.00030000000000000003, 'weight_decay': 0}

